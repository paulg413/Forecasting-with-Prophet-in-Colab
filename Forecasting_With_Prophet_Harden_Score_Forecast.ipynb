{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Forecasting With Prophet - Harden Score Forecast.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZaM+/Q6oFjPj0vvBxFHyx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paulg413/Forecasting-with-Prophet-in-Colab/blob/main/Forecasting_With_Prophet_Harden_Score_Forecast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqILGqSHo5Fo"
      },
      "source": [
        "**Forecasting in Python using Prophet - Tutorial**\n",
        "\n",
        "\n",
        "**Introduction **\n",
        "\n",
        "\n",
        "Recently, I learned about Prophet (fbprophet). If you’re a Data Scientist who works with time-series data, you will love this tool.\n",
        "\n",
        "For those not aware, Prophet was developed by Facebook to aid Data Scientists with automated forecasting for time-series data through its simple Sk-Learn style API.\n",
        "\n",
        "Prophet can be fine-tuned by a data scientist to achieve more specificity. It is an additive forecasting model, and assumed that seasonal effects will be similar each year. Therefore, it doesn’t take a lot into account, but its accuracy can be improved over time through multiple feedback mechanisms. Its website states that Prophet works best with time-series data that has regular seasonality components and lots of historical data to refer to.\n",
        "Since it is open-source, anyone can download and use Prophet. \n",
        "\n",
        "Rohan Gupta Dec 8 2019"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e_x37-jpz5b"
      },
      "source": [
        "**Students Note: The text information below is in case you want to adapt the code to run in another IDE such as VS Studio code, I have adapted this code to run in Colab**\n",
        "\n",
        "Installation\n",
        "I won’t go into much detail here, but since I did have some issues downloading Prophet for the first time, I’ll explain what I did to install it properly:\n",
        "Firstly, you can try a simple pip install if that works:\n",
        "pip install fbprophet\n",
        "However, fbprophet has one major dependency that may cause problems: pystan\n",
        "Ideally, you want to pip install pystan before fbprophet :\n",
        "https://pystan.readthedocs.io/en/latest/installation_beginner.html\n",
        "WINDOWS: pystan needs a compiler. Follow instructions here.\n",
        "The easiest way is to install Prophet in anaconda. Use conda install gcc to set up gcc. Then install Prophet is through conda-forge:\n",
        "conda install -c conda-forge fbprophet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxl4_X1BOVSx"
      },
      "source": [
        "#We will be using the following libraries for the tutorial. All basic libraries except Prophet.\n",
        "\n",
        "import fbprophet\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from fbprophet import Prophet\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jUCrHCWqFGL"
      },
      "source": [
        "For Dataset, I downloaded almost 10 years of game data for James Harden from https://www.basketball-reference.com/players/h/hardeja01.html. Essentially, we have data for almost every single game Harden has played (both in the Regular Season, as well as in the Playoffs.)\n",
        "To use the same data that I’m using, head over to this GitHub https://github.com/dataxienxe/NBAplayersdata page and download the CSV files.\n",
        "\n",
        "\n",
        "Below, I have imported each of the CSV files containing data on James Harden. ‘19rs’ would be “Regular Season 2019–2020” and ‘18po’ would be “Playoffs 2018–2019” to give you an idea of how each file is named."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Et4yvzzTQHXo"
      },
      "source": [
        "#Note, for speed you can select/highlight all files you want to upload in a directory in one go \n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oswhpm2EPoX_"
      },
      "source": [
        "harden19rs = pd.read_csv('harden19rs.csv')\n",
        "harden19po = pd.read_csv('harden19po.csv')\n",
        "harden18rs = pd.read_csv('harden18rs.csv')\n",
        "harden18po = pd.read_csv('harden18po.csv')\n",
        "harden17rs = pd.read_csv('harden17rs.csv')\n",
        "harden17po = pd.read_csv('harden17po.csv')\n",
        "harden16rs = pd.read_csv('harden16rs.csv')\n",
        "harden16po = pd.read_csv('harden16po.csv')\n",
        "harden15rs = pd.read_csv('harden15rs.csv')\n",
        "harden15po = pd.read_csv('harden15po.csv')\n",
        "harden14rs = pd.read_csv('harden14rs.csv')\n",
        "harden14po = pd.read_csv('harden14po.csv')\n",
        "harden13rs = pd.read_csv('harden13rs.csv')\n",
        "harden13po = pd.read_csv('harden13po.csv')\n",
        "harden12rs = pd.read_csv('harden12rs.csv')\n",
        "harden12po = pd.read_csv('harden12po.csv')\n",
        "harden11rs = pd.read_csv('harden11rs.csv')\n",
        "harden11po = pd.read_csv('harden11po.csv')\n",
        "harden10rs = pd.read_csv('harden10po.csv')\n",
        "harden10po = pd.read_csv('harden10rs.csv')\n",
        "harden09rs = pd.read_csv('harden09rs.csv')"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLRkdFmeqyCE"
      },
      "source": [
        "**Cleaning the Data**\n",
        "\n",
        "The next step is super important as we want to make sure our data contains all the required fields or else we wouldn’t be able to do much with it.\n",
        "Let’s start off by appending each of the CSV files. We can make them chronological as there’s a date component. First, we append each file in the following way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV95pkkDRi8e"
      },
      "source": [
        "harden = harden19rs.append(harden19po, ignore_index=True, sort=True)\n",
        "harden = harden.append(harden18rs, ignore_index=True, sort=True)\n",
        "harden = harden.append(harden18po, ignore_index=True, sort=True)\n",
        "harden = harden.append(harden17rs, ignore_index=True, sort=True)\n",
        "harden = harden.append(harden17po, ignore_index=True, sort=True)\n",
        "harden = harden.append(harden16rs, ignore_index=True, sort=True)\n",
        "harden = harden.append(harden16po, ignore_index=True, sort=True)\n",
        "harden = harden.append(harden15rs, ignore_index=True, sort=True)\n",
        "harden = harden.append(harden15po, ignore_index=True, sort=True)\n",
        "harden = harden.append(harden14rs, ignore_index=True, sort=True)\n",
        "harden = harden.append(harden14po, ignore_index=True, sort=True)\n",
        "harden = harden.append(harden13rs, ignore_index=True, sort=True)\n",
        "harden = harden.append(harden13po, ignore_index=True, sort=True)\n",
        "harden = harden.append(harden12rs, ignore_index=True, sort=True)\n",
        "harden = harden.append(harden12po, ignore_index=True, sort=True)\n",
        "harden = harden.append(harden11rs, ignore_index=True, sort=True)\n",
        "harden = harden.append(harden11po, ignore_index=True, sort=True)\n",
        "harden = harden.append(harden10rs, ignore_index=True, sort=True)\n",
        "harden = harden.append(harden10po, ignore_index=True, sort=True)\n",
        "harden = harden.append(harden09rs, ignore_index=True, sort=True)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvNIn3dEq5WT"
      },
      "source": [
        "**Next, I’ll rename some columns so we can understand what they are, and drop the unnecessary columns that aren’t required:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NhGF5S8il3S"
      },
      "source": [
        "harden = harden.rename(columns={'Unnamed: 7': 'Game', 'MP':'Mins'})\n",
        "#harden['Game'] = pd.concat([harden['Unnamed: 7'].dropna(), harden['Game'].dropna()]).reindex_like(harden)\n",
        "#harden = harden.drop(columns=['Unnamed: 7'])\n",
        "harden = harden.drop(columns=['Unnamed: 5']) \n",
        "harden = harden.drop(columns=['▲'])\n",
        "harden = harden.sort_values(by=['Date'])\n",
        "harden = harden.reset_index(drop=True)\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mkAY278rFiT"
      },
      "source": [
        "**The final dataset looks like this: **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWAlhgV0rM9j"
      },
      "source": [
        "harden\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Lg8tAisrkw4"
      },
      "source": [
        "**Setting the Index and Dropping nulls**\n",
        "\n",
        "Since we want the data to be in chronological order, we set the date as our index value and change the datatype to a pandas datetime variable:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stSKFk-qm4Jb"
      },
      "source": [
        "harden.set_index('Date')\n",
        "harden['Date'] = pd.to_datetime(harden['Date'])"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4S0NXCcsP85"
      },
      "source": [
        "Next, we want to drop any null values in our data (even though I doubt we have any). We will use the pandas dropna function and will drop a row in which all values are null. Once we do that, we’ll reset the index to make sure it’s still coherent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxqMOsOwnPWU"
      },
      "source": [
        "harden = harden.dropna(how='all')\n",
        "harden = harden.reset_index(drop=True)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPhZMO4Tsw3y"
      },
      "source": [
        "Next, we have another important step in the data cleanup. During some games, James Harden was inactive, suspended, did not play or did not dress. Certain columns would have these values “suspended” rather than the numeric value that should be there.\n",
        "Thus, we can remove all the rows (games) in which Harden did not play/dress or was inactive/suspended. However, that might reduce the size of my data by a lot. So instead, I will replace the values with the median of each respective column.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-Fo_qplnVMW"
      },
      "source": [
        "for i in harden:\n",
        "    harden[i] = harden[i].replace('Inactive', \n",
        "    np.median(pd.to_numeric(harden[i], errors='coerce')))\n",
        "    \n",
        "for i in harden:\n",
        "    harden[i] = harden[i].replace('Did Not Play', \n",
        "    np.median(pd.to_numeric(harden[i], errors='coerce')))\n",
        "    \n",
        "for i in harden:\n",
        "    harden[i] = harden[i].replace('Did Not Dress', \n",
        "    np.median(pd.to_numeric(harden[i], errors='coerce')))\n",
        "    \n",
        "for i in harden:\n",
        "    harden[i] = harden[i].replace('Player Suspended', \n",
        "    np.median(pd.to_numeric(harden[i], errors='coerce')))\n",
        "    \n",
        "harden = harden.dropna(how='any')\n",
        "harden = harden.reset_index(drop=True)\n",
        "harden.set_index('Date')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLs4gi6MtKFh"
      },
      "source": [
        "**Make sure your data types are on point**\n",
        "\n",
        "\n",
        "A column containing numeric data should be explicitly assigned that datatype, in order to avoid errors in the future. Therefore, I will assign each column its correct data type. Using downcast='float' for float (decimal) columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amaQ6xtoneVv"
      },
      "source": [
        "harden['3P'] = pd.to_numeric(harden['3P'])\n",
        "harden['3PA'] = pd.to_numeric(harden['3PA'])\n",
        "harden['AST'] = pd.to_numeric(harden['AST'])\n",
        "harden['BLK'] = pd.to_numeric(harden['BLK'])\n",
        "harden['DRB'] = pd.to_numeric(harden['DRB'])\n",
        "harden['ORB'] = pd.to_numeric(harden['ORB'])\n",
        "harden['FG'] = pd.to_numeric(harden['FG'])\n",
        "harden['FGA'] = pd.to_numeric(harden['FGA'])\n",
        "harden['PTS'] = pd.to_numeric(harden['PTS'])\n",
        "harden['PF'] = pd.to_numeric(harden['PF'])\n",
        "harden['TOV'] = pd.to_numeric(harden['TOV'])\n",
        "harden['STL'] = pd.to_numeric(harden['STL'])\n",
        "harden['TRB'] = pd.to_numeric(harden['TRB'])\n",
        "harden['3P%'] = pd.to_numeric(harden['3P%'], downcast='float')\n",
        "harden['FG%'] = pd.to_numeric(harden['FG%'], downcast='float')\n",
        "harden['FT%'] = pd.to_numeric(harden['FT%'], downcast='float')\n",
        "harden['GmSc'] = pd.to_numeric(harden['GmSc'], downcast='float')\n",
        "harden['FTA'] = pd.to_numeric(harden['FTA'])\n",
        "harden['FT'] = pd.to_numeric(harden['FT'])\n",
        "print(harden.dtypes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxvlEx7ftmPe"
      },
      "source": [
        "**Forecasting the Data**\n",
        "\n",
        "\n",
        "Now to the best part. We will forecast any column in this dataset using Prophet. The output will give us an image (graph) of the forecast. In order to create the graph, we need to first fit the Prophet model to our dataset. We will separate the column we want, along with the date column. In this case, I’m predicting points, so I’ll take the ‘PTS’ and ‘Date’ columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBFC48WnnvBY"
      },
      "source": [
        "df = harden[['PTS', 'Date']]"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iL_O5MdtwpC"
      },
      "source": [
        "**Fitting**\n",
        "\n",
        "These next steps are quite important, as we will be fitting the prophet model to our data. We want to rename our columns to ‘ds’ (date) and ‘y’ (target). We then define our Prophet model with any given interval_width. Then we fit our Prophet model to the dataset with the date and target variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV1FE7g3n1GU"
      },
      "source": [
        "jh = df.rename(columns={'Date': 'ds', 'PTS': 'y'})\n",
        "jh_model = Prophet (interval_width=0.95)\n",
        "jh_model.fit(jh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUr5dx_suF7X"
      },
      "source": [
        "**To forecast values**, we use the make_future_dataframe function, specify the number of periods, frequency as ‘MS’, which is Multiplicative Seasonality.\n",
        "We then create our matplotlib figure for the forecast. The image below the code shows you the output. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNxOlz8moEMp"
      },
      "source": [
        "jh_forecast = jh_model.make_future_dataframe(periods=36, freq='MS')\n",
        "jh_forecast = jh_model.predict(jh_forecast)\n",
        "plt.figure(figsize=(18, 6))\n",
        "jh_model.plot(jh_forecast, xlabel = 'Date', ylabel = 'PTS')\n",
        "plt.title('James Harden Points')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}